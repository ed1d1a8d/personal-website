<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.150.0"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Tony Wang</title><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=stylesheet href=/css/style.css><script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]]}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body><center>Tony Wang's Personal Website</center><ul class=menu><li><a href=/>Home</a></li><li><a href=/writing/>Writing</a></li><li><a href=/about/>About</a></li></ul><hr><p>Welcome to my personal website.
I am a member of technical staff at the US Center for AI Standards and Innovation and a PhD candidate at MIT.
The overall goal of my work and research is to enable humanity to realize the benefits of advanced AI while adequately managing its downsides.</p><h3 id=research-interests>Research Interests</h3><p>Much of my previous work and thinking has been on adversarial robustness. I&rsquo;ve thought about the phenomenon both in <a href=https://dspace.mit.edu/handle/1721.1/139041 target=_blank>simplified toy settings</a> as well as in the setting of <a href=https://arxiv.org/abs/2211.00241 target=_blank>superhuman</a> <a href=https://arxiv.org/abs/2406.12843 target=_blank>game-playing agents</a>. I&rsquo;m interested in adversarial robustness for two key reasons:</p><ul><li><p>Adversarial robustness is very closely related to the worst-case performance of a system. Safe systems are ones which by definition have acceptable worst-case performance, so adversarial methods can serve as both a good auditing mechanism and as a training signal for safety.</p></li><li><p>Our inability to steer advanced AI systems in a robust way reflects our inability to control the &ldquo;core&rdquo; values and tendencies of AI systems. For example, when RLHF is conducted on a LLM, it behaves aligned in the average case, but the existence of phenomena like &ldquo;jailbreaks&rdquo; shows that we have not effectively changed the &ldquo;true&rdquo; values of the system. Rather we have only instilled in it a bunch of heuristics that make it behave nice most of the time.</p><p>I think working on robustness is a good way to improve our ability to do alignment of &ldquo;core values&rdquo;.</p><p>It might also be that this goal is ill-formed &ndash; the intuition that generally intelligent systems have core values may be false and maybe the <a href=https://sohl-dickstein.github.io/2023/03/09/coherence.html target=_blank>hot-mess theory of intelligence</a> is right. Even in this pessimistic case, studying robustness could still give us highly decision-relevant knowledge about the nature of intelligence.</p></li></ul><p>At the moment, I&rsquo;m focusing on robustness in the vision domain as a stepping stone to robustness more broadly. My key focus is on developing techniques that can improve robustness against unrestricted adversaries. In the vision domain, I am particularly interested in how we can make progress on something like the <a href=https://github.com/openphilanthropy/unrestricted-adversarial-examples target=_blank>Unrestricted Adversarial Examples Challenge</a>.</p><p>In the language domain, the question that interests me the most is this one: Given a natural language specification for how an AI system should behave, how can we build capable systems that robustly satisfy the specification? Ideas related to this question that interest me include <a href=https://openai.com/index/introducing-the-model-spec/ target=_blank>model specs</a>, scalable oversight, <a href=https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training target=_blank>relaxed adversarial training</a>, <a href=https://arxiv.org/abs/2406.04313 target=_blank>representation engineering</a>, <a href=https://arxiv.org/abs/1907.05587 target=_blank>stateful defenses against adversaries</a>, and <a href=https://arxiv.org/abs/2312.06942 target=_blank>AI control</a>.</p><p>Finally, a new topic I have been exploring recently is the ability of AI systems to introspect on their own cognition. More to come on this soon.</p><h3 id=contact>Contact</h3><p>If you would like to chat with me about the topics mentioned on this site,
please contact me at twang6 [at] mit [dot] edu.</p><p>Some links:
<a href=https://twitter.com/TonyWangIV target=_blank>Twitter</a>,
<a href="https://scholar.google.com/citations?user=YWiob00AAAAJ" target=_blank>Google Scholar</a>,
<a href=docs/tony-wang-cv.pdf>CV</a>.</p><hr></body></html>