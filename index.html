<!doctype html><html lang=en-us><head><meta name=generator content="Hugo 0.121.1"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Tony Wang
</title><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=stylesheet href=/css/style.css><script>MathJax={tex:{inlineMath:[["$","$"]],displayMath:[["$$","$$"]]}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script></head><body><center>Tony Wang's Personal Website</center><ul class=menu><li><a href=/>Home</a></li><li><a href=/posts/>Posts</a></li><li><a href=/about/>About</a></li></ul><hr><p>Welcome to my personal website.
I&rsquo;m a 3rd year PhD student at MIT under the supervision of Nir Shavit. I mostly spend my time thinking about how humanity can achieve the upsides of AGI without greatly harming itself in its attempt to do so.</p><h3 id=research-interests>Research Interests</h3><p>Much of my previous work and thinking has been on adversarial robustness. I&rsquo;ve thought about the phenomenon both in <a href=https://dspace.mit.edu/handle/1721.1/139041 target=_blank>very simplified toy settings</a> as well as in the setting of <a href=https://arxiv.org/abs/2211.00241 target=_blank>superhuman game-playing agents</a>. I&rsquo;m interested in adversarial robustness for two key reasons:</p><ul><li><p>Adversarial robustness is very closely related to the worst-case performance of a system. Safe systems are ones which by definition have acceptable worst-case performance, so adversarial methods can serve as both a good auditing mechanism and as a training signal for safety.</p></li><li><p>Our inability to steer advanced AI systems in a robust way reflects our inability to control the &ldquo;core&rdquo; values and tendencies of AI systems. For example, when OpenAI RLHF&rsquo;s GPT-4, it behaves aligned in the average case, but the existence of jailbreaks shows that we have not effectively changed the &ldquo;true&rdquo; values of the system. Rather we have only instilled in it a bunch of heuristics that make it behave nice most of the time.</p><p>I think working on robustness is a good way to improve our ability to do alignment of &ldquo;core values&rdquo;.</p><p>It might also be that this goal is ill-formed &ndash; the intuition that generally intelligent systems have core values may be false and maybe the <a href=https://sohl-dickstein.github.io/2023/03/09/coherence.html target=_blank>hot-mess theory of intelligence</a> is right. Even in this pessimistic case, studying robustness could still give us highly decision-relevant knowledge about the nature of intelligence.</p></li></ul><p>At the moment, I&rsquo;m working on robustness in both the vision and language domains. My key focus is on developing techniques that can improve robustness against unrestricted adversaries. In the vision domain, I am particularly interested in how we can make progress on something like the <a href=https://github.com/openphilanthropy/unrestricted-adversarial-examples target=_blank>Unrestricted Adversarial Examples Challenge</a>. In the language domain (where most of my efforts are right now), I want to answer the following question:</p><p style=text-align:center><b>What are the core difficulties with preventing jailbreaks in language models,<br>and how can these difficulties be overcome?</b></p><p>My current take is that the answer involves scalable oversight and possibly <a href=https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training target=_blank>relaxed adversarial training</a>.</p><p>A north star for my research agenda is to develop techniques that could let us
reliably instill Asimov&rsquo;s three laws into future AGI systems.</p><h3 id=contact>Contact</h3><p>If you would like to chat about anything I&rsquo;ve mentioned on this site,
feel free to contact me at
twang6 [at] mit [dot] edu.</p><p>Some links:
<a href=https://twitter.com/5kovt target=_blank>Twitter</a>,
<a href="https://scholar.google.com/citations?user=YWiob00AAAAJ" target=_blank>Google Scholar</a>,
<a href=docs/tony-wang-cv.pdf>CV</a>.</p><hr></body></html>